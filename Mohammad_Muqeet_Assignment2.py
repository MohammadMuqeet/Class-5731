# -*- coding: utf-8 -*-
"""INFO5731_Assignment_2-1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12HCCZMwIZyXeUNeC7sPis4EByFqW2nej

# **INFO5731 Assignment 2**

In this assignment, you will work on gathering text data from an open data source via web scraping or API. Following this, you will need to clean the text data and perform syntactic analysis on the data. Follow the instructions carefully and design well-structured Python programs to address each question.

**Expectations**:
*   Use the provided .*ipynb* document to write your code & respond to the questions. Avoid generating a new file.
*   Write complete answers and run all the cells before submission.
*   Make sure the submission is "clean"; *i.e.*, no unnecessary code cells.
*   Once finished, allow shared rights from top right corner (*see Canvas for details*).

* **Make sure to submit the cleaned data CSV in the comment section - 10 points**

**Total points**: 100

**Deadline**: Wednesday, at 11:59 PM.

**Late Submission will have a penalty of 10% reduction for each day after the deadline.**

# Question 1 (40 points)

Write a python program to collect text data from **either of the following sources** and save the data into a **csv file:**

(1) Collect all the customer reviews of a product (you can choose any porduct) on amazon. [atleast 1000 reviews]

(2) Collect the top 1000 User Reviews of a movie recently in 2023 or 2024 (you can choose any movie) from IMDB. [If one movie doesn't have sufficient reviews, collect reviews of atleast 2 or 3 movies]

(3) Collect all the reviews of the top 1000 most popular software from G2 or Capterra.

(4) Collect the **abstracts** of the top 10000 research papers by using the query "machine learning", "data science", "artifical intelligence", or "information extraction" from Semantic Scholar.

(5) Collect all the information of the 904 narrators in the Densho Digital Repository.
"""

import requests
from bs4 import BeautifulSoup
import csv

def get_movie_reviews(movie_id, num_reviews=1000):
    reviews = []
    base_url = f"https://www.imdb.com/title/{movie_id}/reviews"

    for start in range(1, num_reviews + 1, 10):
        url = f"{base_url}?sort=submissionDate&dir=desc&ratingFilter=0&ref_=tt_ov_rt"
        if start != 1:
            url += f"&start={start}"

        response = requests.get(url)
        soup = BeautifulSoup(response.content, 'html.parser')
        review_containers = soup.find_all('div', class_='text show-more__control')

        for container in review_containers:
            review_text = container.text.strip()
            reviews.append(review_text)

    return reviews

def save_reviews_to_csv(reviews, filename):
    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(['Review'])
        for review in reviews:
            writer.writerow([review])

# Example usage:
movie_id = "tt0468569"  # Example movie: The Dark Knight
reviews = get_movie_reviews(movie_id)

# Display the collected reviews
for idx, review in enumerate(reviews, 1):
    print(f"Review {idx}:")
    print(review)
    print("=" * 50)

"""# Question 2 (30 points)

Write a python program to **clean the text data** you collected in the previous question and save the clean data in a new column in the csv file. The data cleaning steps include: [Code and output is required for each part]

(1) Remove noise, such as special characters and punctuations.

(2) Remove numbers.

(3) Remove stopwords by using the stopwords list.

(4) Lowercase all texts

(5) Stemming.

(6) Lemmatization.
"""

# Write your code here
!pip install nltk
import nltk
# nltk.download('data')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')
import numpy as np
import nltk.corpus
from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import re

import nltk
import numpy as np
import pandas as pd
import re

nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')

from nltk.corpus import stopwords
from nltk.stem.snowball import SnowballStemmer
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer

stopword_list = stopwords.words('english')
snow_stemmer = SnowballStemmer("english")
lemmatizer = WordNetLemmatizer()

def clean_text(text):
    # Remove special characters, URLs, mentions, and digits
    text = re.sub(r"(@[A-Za-z0-9]+)|([^0-9A-Za-z \t])|(\w+:\/\/\S+)|^rt|http.+?", "", text)
    text = re.sub(r"\d+", "", text)

    # Convert to lowercase
    text = text.lower()

    # Tokenize
    tokens = word_tokenize(text)

    # Remove stopwords
    tokens = [word for word in tokens if word not in stopword_list]

    # Stemming
    stemmed_tokens = [snow_stemmer.stem(word) for word in tokens]

    # Lemmatization
    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]

    return stemmed_tokens, lemmatized_tokens

# Example usage:
reviews = [
    "This movie is amazing and I loved it!",
    "The acting was terrible, but the plot was interesting."
]

cleaned_reviews = []
for review in reviews:
    stemmed, lemmatized = clean_text(review)
    cleaned_reviews.append({
        "Review": review,
        "Stemmed": stemmed,
        "Lemmatized": lemmatized
    })

# Display cleaned reviews
for idx, review in enumerate(cleaned_reviews, 1):
    print(f"Review {idx}:")
    print("Original:", review["Review"])
    print("Stemmed:", review["Stemmed"])
    print("Lemmatized:", review["Lemmatized"])
    print("=" * 50)

"""# Question 3 (30 points)

Write a python program to **conduct syntax and structure analysis of the clean text** you just saved above. The syntax and structure analysis includes:

(1) **Parts of Speech (POS) Tagging:** Tag Parts of Speech of each word in the text, and calculate the total number of N(oun), V(erb), Adj(ective), Adv(erb), respectively.

(2) **Constituency Parsing and Dependency Parsing:** print out the constituency parsing trees and dependency parsing trees of all the sentences. Using one sentence as an example to explain your understanding about the constituency parsing tree and dependency parsing tree.

(3) **Named Entity Recognition:** Extract all the entities such as person names, organizations, locations, product names, and date from the clean texts, calculate the count of each entity.
"""

!python -m spacy download en_core_web_sm

# Commented out IPython magic to ensure Python compatibility.
import nltk
nltk.download('averaged_perceptron_tagger')
import spacy
nlp=spacy.load('en_core_web_sm')
# %tensorflow_version 2.x
!pip install benepar
import benepar
benepar.download('benepar_en2')
from benepar.spacy_plugin import BeneparComponent
benepar.download('benepar_en3')

import spacy
nlp = spacy.load("en_core_web_sm")  # Load English language model

def syn_struc(tokens):
    pos_tags = []
    for token in tokens:
        doc = nlp(token)
        for t in doc:
            pos_tags.append([t.text, t.pos_, t.tag_])
    return pos_tags

# Assuming df and 'tokens' column are defined
df['pos'] = df['tokens'].apply(lambda x: syn_struc(x))
print(df['pos'].head())
print(len(df['pos']))

"""# Mandatory Question

Provide your thoughts on the assignment. What did you find challenging, and what aspects did you enjoy? Your opinion on the provided time to complete the assignment.
"""

Doing this assignment was both challenging and rewarding. Deciphering the specific requirements demanded careful attention to ensure alignment with the expected outcomes. Addressing errors, particularly in code implementation, required meticulous debugging and troubleshooting to ensure smooth execution. Additionally, preprocessing the provided data proved time-consuming, as tasks such as cleaning and formatting were essential for accurate analysis.

Despite these challenges, engaging with the assignment provided a valuable opportunity for learning and growth. Problem-solving throughout the process fostered intellectual stimulation and contributed to skill enhancement. Exploring new concepts, techniques, and libraries facilitated continuous learning, catering to individuals passionate about expanding their knowledge base. Moreover, hands-on experience with real data solidified understanding and retention of key concepts, enhancing the overall learning experience.

Successfully completing the assignment and achieving the desired outcomes instilled a sense of accomplishment and personal satisfaction. It served as a testament to the dedication invested in overcoming challenges and refining solutions. Adequate time allocation was crucial for delving deeply into concepts, addressing challenges effectively, and ultimately delivering high-quality solutions. Overall, the assignment served as a rewarding and enriching experience, contributing to both personal and professional growth.